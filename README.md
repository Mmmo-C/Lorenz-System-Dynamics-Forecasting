# Lorenz-System-Dynamics-Forecasting
Implementation of different nueral network structures on Lorenz System.

</p>
Xinqi Chen @15/05/2023 

## Table of Content
- [Lorenz System Dynamics Forecasting](#lorenz-system-dynamics-forecasting)
  - [Abstract](#abstract)
  - [Overview](#overview)
  - [Theoretical Background](#theoretical-background)
  - [Algorithm Implementation and Development](#algorithm-implementation-and-development)
  - [Computational Results](#computational-results)
  - [Summary and Conclusions](#summary-and-conclusions)
  - [Acknowledgement](#acknowledgement)
  
## Abstract
This project focuses on forecasting the dynamics of the Lorenz system using different neural network architectures, including feed-forward networks, LSTM (Long Short-Term Memory) networks, and RNN (Recurrent Neural Network) networks. The goal is to compare the performance of these models in predicting future states of the system given its initial conditions and system parameters. The project also provides an overview of the theoretical background of the Lorenz system, the neural network architectures used, and the algorithm implementation.

## Ovreview
The Lorenz system is a set of nonlinear ordinary differential equations that describes a simplified model of atmospheric convection. It exhibits chaotic behavior, making it an interesting system to study. In this project, we train neural network models to advance the solution from time t to t + ∆t for different values of the system parameter ρ, and then evaluate the models' performance in predicting future states for unseen ρ values.

## Theoretical Background
The Lorenz system is defined by the following set of equations:
```
dx/dt = sigma * (y - x)
dy/dt = x * (rho - z) - y
dz/dt = x * y - beta * z
```
where x, y, and z represent the state variables, sigma, rho, and beta are system parameters, and t is time.

**Neural network architectures used in this project:**
1. Feed-forward neural networks (FFNNs) capture nonlinear relationships but struggle with temporal dependencies.
2. Long Short-Term Memory networks (LSTMs) are capable of modeling long-term dependencies in time-series data.
3. Recurrent Neural Networks (RNNs) can handle sequential data and capture temporal dependencies.

## Algorithm Implementation and Development
1. Data Generation: Training data is generated by integrating the Lorenz equations for multiple initial conditions and time points.
```ruby
def lorenz_deriv(x_y_z, t0, sigma=sigma, beta=beta, rho=rho):
    x, y, z = x_y_z
    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]
```

2. Neural Network Model Definition: Different neural network models, including feed-forward, LSTM, and RNN models, are defined using Keras.
```ruby
# Feed-forward NN
model = Sequential()
model.add(Dense(64, input_dim=3, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(3))
```

```ruby
# LSTM model
model_lstm = Sequential()
model_lstm.add(LSTM(128, input_shape=(4, 1)))
model_lstm.add(Dense(3))
```

```ruby
# RNN model
model_rnn = Sequential()
model_rnn.add(Dense(128, input_shape=(4,)))
model_rnn.add(Dense(3))
```

3. Training: The models are trained using the generated training data, with mean squared error loss and appropriate optimization algorithms.
```ruby
model.fit(X_train, y_train, epochs=100, batch_size=64)
```

4. Evaluation: The trained models are evaluated by predicting future states for unseen ρ values and calculating the mean squared error.
```ruby
x0_test = -15 + 30 * np.random.random((100, 3))
rho_test = [17, 35]
x_t_test = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t) for x0_j in x0_test])
X_test, y_test = generate_data(x_t_test, dt, rho_test)

# Reshape the test data to match the input shape of the LSTM model
X_test_lstm = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

# Use the trained LSTM model to make predictions on the test data
y_pred_lstm = model_lstm.predict(X_test_lstm)

# Use the trained RNN model to make predictions on the test data
y_pred_rnn = model_rnn.predict(X_test)
```

5. Visualization: The predicted and actual trajectories are visualized using 3D plots.

## Computational Results
First, the Lorenz System is evaluated with fixed parameters and radom data set. The result at rho = 28 is:
![r28]()

As the system is trained with a feed-forward nueral network with traning data set rho = [10, 28, 40], the forecasting on rho = [17, 35] is:
![result]()

The leas square error of LSTM and RNN system using the same test data set is:
```
LSTM MSE: 0.0031942127091748097
RNN MSE: 0.3095720377747974
```

Comparing different types of neural networks for time-series forecasting the dynamics of the Lorenz system, each type has their own strength and weekness.

Feed-forward neural networks:
Feed-forward neural networks are useful for capturing complex nonlinear relationships between the input and output variables. However, they are not well-suited for capturing temporal dependencies in time-series data, and may struggle to generalize to new data.

Recurrent neural networks:
Recurrent neural networks (RNNs) are particularly useful for time-series forecasting because they can model long-term dependencies in the data. However, standard RNNs may suffer from the vanishing gradient problem, which makes it difficult to learn long-term dependencies. Also, RNN has the fastest run time.

Long short-term memory networks:
Long short-term memory networks (LSTM) are particularly well-suited for modeling complex sequences with long-term dependencies, and have been shown to perform well in time-series forecasting tasks.

Echo state networks:
Echo state networks (ESNs) are designed to be fast and easy to train, and have been shown to perform well on a variety of time-series forecasting tasks. However, the performance of ESNs is highly dependent on the choice of network hyperparameters and initialization.

## Summary and Conclusions
Overall, the LSTM and RNN models exhibit better performance in capturing the temporal dependencies and chaotic behavior of the Lorenz system compared to the feed-forward networks. The LSTM networks, in particular, are well-suited for modeling long-term dependencies and are effective in forecasting future states. However, it is important to note that the choice of neural network architecture may depend on the specific requirements and characteristics of the problem at hand.

This project serves as a foundation for further exploration and analysis of neural network models for forecasting dynamical systems. Future work may involve experimenting with different architectures, tuning hyperparameters, and exploring other forecasting techniques to enhance the accuracy and robustness of the models.

## Acknowledgement
- [ChatGPT](https://platform.openai.com/)
- [Instructor: Nathan Kutz](https://github.com/nathankutz)
